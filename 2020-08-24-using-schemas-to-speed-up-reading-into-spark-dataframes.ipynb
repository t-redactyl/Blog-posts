{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Using schemas to speed up reading into Spark DataFrames  \n",
    "date: 2020-08-24   \n",
    "comments: false  \n",
    "tags: pyspark  \n",
    "keywords: python, data science, aws, sagemaker, s3, pyspark\n",
    "\n",
    "---\n",
    "\n",
    "While Spark is the best thing since sliced bread for dealing with big data, I definitely realise I have a lot to learn before I can use it to its full potential. One trick I recently discovered was using explicit schemas to speed up how fast Spark can read a CSV into a DataFrame.\n",
    "\n",
    "When using `spark.read_csv` to read in a CSV, the most straightforward way is to set the `inferSchema` argument to `True`. This means that Spark will attempt to check the data in order to work out what type of data each column is. \n",
    "\n",
    "The problem with this operation is that it's quite memory intensive, especially for large datasets, as Spark needs to look at a sufficient amount of data in order to correctly infer the type. Imagine that you have a column with integers in the first 1000 rows, but then a string in the 1001th row. If Spark had inferred this column was an `IntegerType` based on the top few rows, then we would end up with missing values for each of the rows containing a string. As such, Spark either needs to scan the whole dataset, or randomly sample enough rows to infer the type.\n",
    "\n",
    "One way we can get around this is by determining the type of the data beforehand and passing this information to Spark using an schema. The syntax for this is pretty straightforward. Each column's name and type is defined using the `StructField` method from `pyspark.sql`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StructField(\"trip_id\", StringType(), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three arguments that `StructField` takes are the name you'd like to give the column, the column's data type, and whether the field can contain null values (as a boolean). The available column data types are also in `pyspark.sql`, and cover a wide type of possible data types, from string, float and integer to boolean and datetime. A `StructField` is created for each column, and these are passed as a list to `pyspark.sql`'s `StructType`. This schema can then be passed to `spark.read.csv`'s `schema` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StructType(\n",
    "    [StructField(\"trip_id\", StringType(), True),\n",
    "    StructField(\"call_type\", StringType(), True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test how much faster it is to use a schema, I will be using the [Taxi Service Trajectory](https://archive.ics.uci.edu/ml/datasets/Taxi+Service+Trajectory+-+Prediction+Challenge,+ECML+PKDD+2015) dataset from the UCI Machine Learning Repository. This dataset has 9 columns and around 1.7 million rows.\n",
    "\n",
    "We'll first set up our `SparkSession` to be able to access our data on S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import sagemaker_pyspark\n",
    "import botocore.session\n",
    "\n",
    "from time import time\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, BooleanType\n",
    "\n",
    "session = botocore.session.get_session()\n",
    "credentials = session.get_credentials()\n",
    "\n",
    "conf = (SparkConf()\n",
    "        .set(\"spark.driver.extraClassPath\", \":\".join(sagemaker_pyspark.classpath_jars())))\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(conf=conf) \\\n",
    "    .config('fs.s3a.access.key', credentials.access_key)\n",
    "    .config('fs.s3a.secret.key', credentials.secret_key)\n",
    "    .appName(\"schema_test\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also set up our data path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"sagemaker-pyspark\"\n",
    "data_key = \"train.csv\"\n",
    "data_location = f\"s3a://{bucket}/{data_key}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first time how long it takes to read in these data when Spark has to infer the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed in 28.1013503074646 sec.\n"
     ]
    }
   ],
   "source": [
    "t1 = time()\n",
    "\n",
    "data_inferred = spark.read.csv(data_location, header = 'True', inferSchema = True)\n",
    "\n",
    "t2 = time()\n",
    "print('Completed in %s sec.' % (str(t2 - t1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now compare how long it takes when we explicitly tell Spark what the data schema is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed in 2.506075620651245 sec.\n"
     ]
    }
   ],
   "source": [
    "t1 = time()\n",
    "\n",
    "schema = StructType(\n",
    "    [StructField(\"trip_id\", StringType(), True),\n",
    "    StructField(\"call_type\", StringType(), True),\n",
    "    StructField(\"origin_call\", IntegerType(), True),\n",
    "    StructField(\"origin_stand\", IntegerType(), True),\n",
    "    StructField(\"taxi_id\", LongType(), True),\n",
    "    StructField(\"timestamp\", LongType(), True),\n",
    "    StructField(\"day_type\", StringType(), True),\n",
    "    StructField(\"missing_data\", BooleanType(), True),\n",
    "    StructField(\"polyline\", StringType(), True)]\n",
    ")\n",
    "data_schema = spark.read.csv(data_location, header = 'False', schema = schema)\n",
    "\n",
    "t2 = time()\n",
    "print('Completed in %s sec.' % (str(t2 - t1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are read in around 10 times faster when we give Spark the schema rather than asking it to infer it! Obviously this is a more practical step when you have data with fewer variables, but when reading truly large data, it is an easy way to save some processing time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
