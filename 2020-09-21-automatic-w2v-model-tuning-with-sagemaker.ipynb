{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Automatic word2vec model tuning using Sagemaker  \n",
    "date: 2020-11-18    \n",
    "comments: false  \n",
    "tags: aws, sagemaker, machine learning  \n",
    "keywords: python, data science, aws, sagemaker, s3, pyspark, blazingtext, word2vec, w2v, nlp\n",
    "\n",
    "---\n",
    "\n",
    "In this post, we continue our discussion about how to use AWS Sagemaker's BlazingText to train a word2vec model. In the [last post]({filename}2020-09-07-training-w2v-with-sagemaker-blazing-text.md) we learned how to set up, train and evaluate a single model. However, we essentially selected our hyperparameters at random, meaning our model is not likely to be performing as well as it could. Traditionally, we would either do a [grid search](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search) to exhaustively find the best combination of hyperparameters, or a [random search](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Random_search) to sample from these combinations. \n",
    "\n",
    "Sagemaker offers a third alternative, which is to use a method called [Bayesian optimisation](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Bayesian_optimization). I will not pretend to understand how this works in detail, but the general idea is that, starting from a seed set of hyperparameters, the optimiser checks how each model performs against our objective metric and then tries to pick a set of hyperparameters that will improve the model performance in the next round of training. The tuning then continues until either the model cannot be further improved, or the maximum number of training rounds has been reached. This [excellent video from AWS](https://www.youtube.com/watch?v=xpZFNIOaQns) explains in more detail how it has been implemented in Sagemaker.\n",
    "\n",
    "In this post I'll take you through how to tune a set of word2vec models using Sagemaker's inbuilt objective metric, the WS-353 goldsets, as well as discuss some practical considerations such as the cost of this tuning and the potential limitations of the WS-353 for some NLP tasks.\n",
    "\n",
    "## Setting up our tuning\n",
    "\n",
    "In order to get started, we'll use the exact same set up for the execution role, S3 bucket, training image and Sagemaker estimator which I discussed in the last post. We'll also start with the same set of hyperparameter we used for our last model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tuner import (IntegerParameter, CategoricalParameter, ContinuousParameter, \n",
    "                             HyperparameterTuner)\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket_name = 'sagemaker-blog-corpus-nlp'\n",
    "tags = [{'Key': 'user:application', 'Value': 'BlazingText'}]\n",
    "\n",
    "region_name = boto3.Session().region_name\n",
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "\n",
    "# Create estimator object\n",
    "bt_model = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c5.xlarge',\n",
    "    train_volume_size=11,\n",
    "    train_max_run=36000,\n",
    "    base_job_name='blazingtext-blogs-sentences',\n",
    "    input_mode='File',\n",
    "    output_path='s3://{}/models/blazingtext'.format(bucket_name),\n",
    "    tags=tags\n",
    ")\n",
    "\n",
    "# Set initial hyperparameters\n",
    "bt_model.set_hyperparameters(\n",
    "    mode=\"batch_skipgram\",\n",
    "    epochs=10, \n",
    "    min_count=40,\n",
    "    sampling_threshold=0.0001,\n",
    "    learning_rate=0.05,\n",
    "    window_size=5,\n",
    "    vector_dim=100,\n",
    "    negative_samples=5,\n",
    "    batch_size=11,\n",
    "    evaluation=True,\n",
    "    subwords=False\n",
    ")\n",
    "\n",
    "# Get path to training data\n",
    "input_data = f\"s3://{bucket_name}/cleaned_sentences.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the automatic tuner rather than just training a single model, we need to take a couple of extra steps here. Firstly, we need to decide which hyperparameters we wish to tune and what ranges of possible values we'd like the tuner to test for each one. [This page](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext-tuning.html) gives a guide on the most important hyperparameters and their recommended ranges for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    'mode': CategoricalParameter(['batch_skipgram', 'cbow']),\n",
    "    'learning_rate': ContinuousParameter(0.005, 0.05, scaling_type=\"Logarithmic\"),\n",
    "    'window_size': IntegerParameter(5, 30),\n",
    "    'vector_dim': IntegerParameter(50, 100),\n",
    "    'min_count': IntegerParameter(20, 50),\n",
    "    'negative_samples': IntegerParameter(5,25)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to set up our tuner. We indicate that we want the tuner to use the BlazingText estimator we created, and maximise the mean correlation with the WS-353 gold sets in order to optimise the model. We've also told the tuner to only try to test the hyperparameters we included in our `hyperparameter_ranges` dictionary above. Finally, we've also told the model that we only want to do up to 12 rounds of tuning, and that the model can run up to two training jobs in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'train:mean_rho'\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    bt_model,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    max_jobs=12,\n",
    "    max_parallel_jobs=2,\n",
    "    objective_type='Maximize',\n",
    "    tags=tags\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finally ready to kick off our tuning job! Unlike when training a single job, when we execute this cell we won't get any feedback in the notebook. However, you can go to your Sagemaker dashboard and look at `Hyperparameter Tuning Jobs` under `Training` and you'll be able to see that the job has started. You're also able to shut down your Sagemaker notebook instance while you wait for the tuning job to finish. This is also something you might want to set to run overnight, as the total training time for these models was around 7.5 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit({'train': input_data}, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing our models and picking the best performing\n",
    "\n",
    "Once our tuning job is finished, we can retrieve all of our model hyperparameters and performance metrics using Sagemaker search. The following code will retrieve all models contained in the S3 bucket I am using for this project, as I've asked it to retrieve all models from buckets containing `blog-corpus-nlp`. I took this code directly from [the video on automatic tuning](https://www.youtube.com/watch?v=xpZFNIOaQns) that I discussed at the beginning of this post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "smclient = boto3.client(service_name = \"sagemaker\")\n",
    "\n",
    "search_params = {\n",
    "    \"MaxResults\": 100,\n",
    "    \"Resource\": \"TrainingJob\",\n",
    "    \"SearchExpression\": {\n",
    "        \"Filters\": [\n",
    "            {\n",
    "            \"Name\": \"InputDataConfig.DataSource.S3DataSource.S3Uri\",\n",
    "            \"Operator\": \"Contains\",\n",
    "            \"Value\": \"blog-corpus-nlp\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"TrainingJobStatus\",\n",
    "                \"Operator\": \"Equals\",\n",
    "                \"Value\": \"Completed\"\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "}\n",
    "results = smclient.search(**search_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are returned in a rather messy JSON, so in order to make it a bit easier to check and compare these models, I'll extract the relevant data I want and put it in a `pandas` DataFrame. I'll also rank the table so that the models that performed best against the objective metric are at the top of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>mean_rho</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>min_count</th>\n",
       "      <th>negative_samples</th>\n",
       "      <th>mode</th>\n",
       "      <th>vector_dim</th>\n",
       "      <th>window_size</th>\n",
       "      <th>billableSeconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>blazingtext-200627-1812-010-4a414e1b</td>\n",
       "      <td>0.744891</td>\n",
       "      <td>0.01157812214687356</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>batch_skipgram</td>\n",
       "      <td>100</td>\n",
       "      <td>28</td>\n",
       "      <td>1385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>blazingtext-200627-1812-009-218d44a5</td>\n",
       "      <td>0.744700</td>\n",
       "      <td>0.005359255169841052</td>\n",
       "      <td>38</td>\n",
       "      <td>25</td>\n",
       "      <td>batch_skipgram</td>\n",
       "      <td>100</td>\n",
       "      <td>24</td>\n",
       "      <td>3163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>blazingtext-200627-1812-011-269247ac</td>\n",
       "      <td>0.742675</td>\n",
       "      <td>0.005484088259142749</td>\n",
       "      <td>38</td>\n",
       "      <td>25</td>\n",
       "      <td>batch_skipgram</td>\n",
       "      <td>99</td>\n",
       "      <td>24</td>\n",
       "      <td>3705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>blazingtext-200627-1812-004-88f21ab4</td>\n",
       "      <td>0.737871</td>\n",
       "      <td>0.005853343297007583</td>\n",
       "      <td>36</td>\n",
       "      <td>18</td>\n",
       "      <td>batch_skipgram</td>\n",
       "      <td>80</td>\n",
       "      <td>24</td>\n",
       "      <td>2348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>blazingtext-200627-1812-006-a80fdc2a</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.006415934965973922</td>\n",
       "      <td>49</td>\n",
       "      <td>23</td>\n",
       "      <td>batch_skipgram</td>\n",
       "      <td>65</td>\n",
       "      <td>22</td>\n",
       "      <td>2727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blazingtext-200627-1812-005-0c7e32d9</td>\n",
       "      <td>0.735655</td>\n",
       "      <td>0.005989685177241382</td>\n",
       "      <td>36</td>\n",
       "      <td>18</td>\n",
       "      <td>batch_skipgram</td>\n",
       "      <td>79</td>\n",
       "      <td>24</td>\n",
       "      <td>2492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blazingtext-200627-1812-007-d617ec0b</td>\n",
       "      <td>0.735165</td>\n",
       "      <td>0.0065653812896101695</td>\n",
       "      <td>49</td>\n",
       "      <td>23</td>\n",
       "      <td>batch_skipgram</td>\n",
       "      <td>66</td>\n",
       "      <td>22</td>\n",
       "      <td>2491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blazingtext-200627-1812-012-4b6fa554</td>\n",
       "      <td>0.733655</td>\n",
       "      <td>0.010487437290229438</td>\n",
       "      <td>39</td>\n",
       "      <td>11</td>\n",
       "      <td>batch_skipgram</td>\n",
       "      <td>90</td>\n",
       "      <td>30</td>\n",
       "      <td>1962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blazingtext-200627-1812-002-43d5a552</td>\n",
       "      <td>0.731108</td>\n",
       "      <td>0.02690169433088229</td>\n",
       "      <td>40</td>\n",
       "      <td>17</td>\n",
       "      <td>batch_skipgram</td>\n",
       "      <td>93</td>\n",
       "      <td>23</td>\n",
       "      <td>2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>blazingtext-200627-1812-008-b935f3c1</td>\n",
       "      <td>0.721126</td>\n",
       "      <td>0.005592398208454858</td>\n",
       "      <td>47</td>\n",
       "      <td>9</td>\n",
       "      <td>batch_skipgram</td>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>1289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>blazingtext-200627-1812-001-2eab54f0</td>\n",
       "      <td>0.720144</td>\n",
       "      <td>0.015012459461540008</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>batch_skipgram</td>\n",
       "      <td>77</td>\n",
       "      <td>8</td>\n",
       "      <td>1889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blazingtext-200627-1812-003-c735efa6</td>\n",
       "      <td>0.631023</td>\n",
       "      <td>0.010053755343191226</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>cbow</td>\n",
       "      <td>57</td>\n",
       "      <td>15</td>\n",
       "      <td>769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              model_name  mean_rho          learning_rate  \\\n",
       "8   blazingtext-200627-1812-010-4a414e1b  0.744891    0.01157812214687356   \n",
       "7   blazingtext-200627-1812-009-218d44a5  0.744700   0.005359255169841052   \n",
       "9   blazingtext-200627-1812-011-269247ac  0.742675   0.005484088259142749   \n",
       "11  blazingtext-200627-1812-004-88f21ab4  0.737871   0.005853343297007583   \n",
       "6   blazingtext-200627-1812-006-a80fdc2a  0.736842   0.006415934965973922   \n",
       "0   blazingtext-200627-1812-005-0c7e32d9  0.735655   0.005989685177241382   \n",
       "4   blazingtext-200627-1812-007-d617ec0b  0.735165  0.0065653812896101695   \n",
       "1   blazingtext-200627-1812-012-4b6fa554  0.733655   0.010487437290229438   \n",
       "2   blazingtext-200627-1812-002-43d5a552  0.731108    0.02690169433088229   \n",
       "12  blazingtext-200627-1812-008-b935f3c1  0.721126   0.005592398208454858   \n",
       "10  blazingtext-200627-1812-001-2eab54f0  0.720144   0.015012459461540008   \n",
       "3   blazingtext-200627-1812-003-c735efa6  0.631023   0.010053755343191226   \n",
       "\n",
       "   min_count negative_samples            mode vector_dim window_size  \\\n",
       "8         32                6  batch_skipgram        100          28   \n",
       "7         38               25  batch_skipgram        100          24   \n",
       "9         38               25  batch_skipgram         99          24   \n",
       "11        36               18  batch_skipgram         80          24   \n",
       "6         49               23  batch_skipgram         65          22   \n",
       "0         36               18  batch_skipgram         79          24   \n",
       "4         49               23  batch_skipgram         66          22   \n",
       "1         39               11  batch_skipgram         90          30   \n",
       "2         40               17  batch_skipgram         93          23   \n",
       "12        47                9  batch_skipgram         50          20   \n",
       "10        28               21  batch_skipgram         77           8   \n",
       "3         23               10            cbow         57          15   \n",
       "\n",
       "    billableSeconds  \n",
       "8              1385  \n",
       "7              3163  \n",
       "9              3705  \n",
       "11             2348  \n",
       "6              2727  \n",
       "0              2492  \n",
       "4              2491  \n",
       "1              1962  \n",
       "2              2587  \n",
       "12             1289  \n",
       "10             1889  \n",
       "3               769  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extractModelInfo(model_dict):\n",
    "    model_name = model_dict[\"TrainingJobName\"]\n",
    "    bs = model_dict[\"BillableTimeInSeconds\"]\n",
    "    score = model_dict[\"FinalMetricDataList\"][0][\"Value\"]\n",
    "    hyperparams = model_dict[\"HyperParameters\"]\n",
    "    \n",
    "    d1 = {\"model_name\": model_name, \"billableSeconds\": bs, \"mean_rho\": score}\n",
    "    return {**d1, **hyperparams}\n",
    "\n",
    "desired_fields = [\"model_name\", \"mean_rho\", \"learning_rate\", \"min_count\", \"negative_samples\", \n",
    "                  \"mode\", \"vector_dim\", \"window_size\", \"billableSeconds\"]\n",
    "\n",
    "results_df = pd.DataFrame([extractModelInfo(results[\"Results\"][i][\"TrainingJob\"]) \n",
    "                           for i in np.arange(1, len(results[\"Results\"]))])\n",
    "results_df = results_df.loc[results_df[\"mode\"] != \"skipgram\", \n",
    "                            desired_fields].sort_values(\"mean_rho\", ascending = False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the best performing model used a skipgram architecture, had a learning rate of 0.01, a minimum count for each term of 32, used 6 negative samples, used a 100 dimension vector size and used a rather generous window size of 28 for checking proximate words. It also outperformed our previous model, with a `mean_rho` score of 0.74 (compared to 0.72 for the model we trained in the last post).\n",
    "\n",
    "Let's check the neighbours we get from this model for the same two queries \"family\" and \"sad\" that we checked for the untuned model from the last post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://sagemaker-blog-corpus-nlp/models/blazingtext/blazingtext-200627-1812-010-4a414e1b/output/model.tar.gz - | tar -xz    \n",
    "!pip install gensim\n",
    "    \n",
    "from gensim.models import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format('data/best_tuning_model/vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('relatives', 0.7486746907234192),\n",
       " ('grandparents', 0.7312743663787842),\n",
       " ('uncles', 0.7192485928535461),\n",
       " ('cousins', 0.7149008512496948),\n",
       " ('aunts', 0.7009657621383667),\n",
       " ('parents', 0.6931362152099609),\n",
       " ('aunt', 0.6655760407447815),\n",
       " ('grandpa', 0.6604502201080322),\n",
       " ('nephews', 0.6573513150215149),\n",
       " ('grandmother', 0.6546939611434937)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(\"family\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('saddening', 0.7043677568435669),\n",
       " ('depressing', 0.6888615489006042),\n",
       " ('happy', 0.676035463809967),\n",
       " ('unhappy', 0.6721015572547913),\n",
       " ('depressed', 0.6692825555801392),\n",
       " ('cry', 0.6438484191894531),\n",
       " ('pathetic', 0.6382129192352295),\n",
       " ('upset', 0.6326898336410522),\n",
       " ('angry', 0.6272262930870056),\n",
       " ('heartbroken', 0.6254571080207825)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(\"sad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the neighbours look around as good for \"family\" as for the previous model, but seem to look a little better for \"sad\", showing more relevant terms and less typos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How much did this training cost?\n",
    "\n",
    "I was a little worried when I first started this experiment at home, as I was concerned that running a Sagemaker ML training instance for over 7 hours might be really expensive! Happily, the `ml.c5.xlarge` instances are surprisingly affordable, priced at US\\$0.272 per hour in the Frankfurt region. The Sagemaker notebook instances are also reasonably priced, costing US\\$0.0536 per hour. My total bill for running this entire training job is US\\$2.84. Just remember to shut down your Sagemaker notebook instances when you're not using them, as they can add up quickly if you forget about them for a couple of days!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few final thoughts about the WS-353 as a metric\n",
    "\n",
    "As you can see, for this general-purpose, English language dataset, the WS-353 metrics seemed to work very well for helping us evaluate and tune our word2vec models. However, there are some issues that some with using these datasets which you might want to be aware of for your own NLP projects. I encountered a few of these issues while using the Sagemaker automatic tuning process at my previous job.\n",
    "\n",
    "Firstly, the WS-353 goldsets are entirely in English, and from what I could find out there is no option to specify the language of the training data and adjust the gold sets used to evaluate the model accordingly. This really limits their utility for non-English language NLP tasks.\n",
    "\n",
    "Secondly, given that the associations between word pairs in the WS-353 are general, they may not capture associations in more specific domains. For example, in a general context, \"java\" might represent coffee and \"python\" might present a snake, and therefore be considered completely unrelated. However, in a job context, \"Java\" and \"Python\" would be much more likely to represent programming languages and therefore be quite similar.\n",
    "\n",
    "In addition, [this interesting paper](https://arxiv.org/pdf/1605.02276.pdf) lists a number of other potential limitations of using the WS-353 for tuning, including issues with averaging over similarity and relatedness (which can semantically represent quite different things) and the potential for overfitting to these small datasets.\n",
    "\n",
    "These considerations definitely don't invalidate the use of the BlazingText objective metric (well, at least for English-language tasks); however, like everything in data science it means that the tuning process should not be treated as a silver bullet, but should be evaluated within the context of the NLP task you have.\n",
    "\n",
    "I hope this and the previous article gave you a practical overview on how to train word2vec models using BlazingText, its advantages over training locally, and how to potentially leverage the automatic tuning process to make a better performing model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
